"""Benchmark offline inference throughput."""
import argparse
import json
import random
import time
from typing import List, Optional, Tuple

import torch
from transformers import (AutoModelForCausalLM, AutoTokenizer,
                          PreTrainedTokenizerBase)
from tqdm import tqdm


def run_hf(
    requests: List[Tuple[str, int, int]],
    model: str,
    tokenizer: PreTrainedTokenizerBase,
    trust_remote_code: bool,
) -> float:

    # tokenizer.pad_token = tokenizer.eos_token
    # output = torch.Tensor([0, 18637, 29892, 526, 366, 1136, 455, 2470, 29973, 1815, 366, 5193, 304, 592, 29973, 13, 13, 22550, 29901, 13, 13, 29950, 1032, 727, 29991, 306, 29915, 29885, 925, 385, 319, 29902, 29892, 306, 1016, 29915, 29873, 505, 7333, 27482, 470, 23023, 1080, 763, 25618, 437, 29892]).int().cuda()
    # output = torch.Tensor([0, 18637, 29892, 526, 366, 1136, 455, 2470, 29973, 1815, 366, 5193, 304, 592, 29973, 13, 13, 22550, 29901, 13, 13, 29950, 1032, 727, 29991, 306, 29915, 29885, 925, 385, 319, 29902, 29892, 306, 1016, 29915, 29873, 505, 7333, 27482, 470, 23023, 1080, 763, 25618, 437, 29892, 541, 306, 29915, 29885, 1244, 304, 1371, 366, 411, 738, 5155, 470, 4828, 366, 1795, 505, 29889, 1317, 727, 1554, 2702, 366, 29915, 29881, 763, 304, 5193, 1048, 470, 2244, 29973, 2]).int().cuda()
    # output = torch.Tensor([0, 18637, 29892, 526, 366, 1136, 455, 2470, 29973, 1815, 366, 5193, 304, 592, 29973, 13, 13, 22550, 29901, 13, 13, 29950, 1032, 727, 29991, 306, 29915, 29885, 925, 385, 319, 29902, 29892, 306, 1016, 29915, 29873, 505, 7333, 27482, 470, 23023, 1080, 763, 25618, 437, 29892, 541, 306, 29915, 29885, 1244, 304, 1371, 366, 411, 738, 5155, 470, 4828, 366, 1795, 505, 29889, 1317, 727, 1554, 2702, 366, 29915, 29881, 763, 304, 5193, 1048, 470, 2244, 29973, 2, 29873, 2235, 304, 592, 2, 29873, 2235, 304, 592, 13, 13, 2, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]).int().cuda()
    output = torch.Tensor([1, 306, 1074, 29892, 3969, 363, 278, 7542, 2450, 29889, 512, 278, 1857, 5314, 29892, 372, 338, 1950, 363, 278, 421, 17158, 29918, 8172, 29918, 1272, 2555, 740, 304, 5706, 7929, 4206, 1363, 278, 4036, 848, 338, 5759, 25499, 363, 1269, 1948, 29889, 2398, 29892, 278, 6976, 310, 14655, 7929, 4206, 338, 1407, 4482, 1363, 372, 6858, 278, 2684, 1021, 1819, 304, 367, 5759, 363, 599, 4341, 29889, 512, 6944, 29892, 372, 338, 10812, 304, 11735, 1565, 7929, 4206, 297, 263, 8783, 29889, 13, 13, 3644, 366, 864, 304, 9801, 393, 727, 526, 7929, 4206, 297, 278, 8783, 29892, 366, 508, 6623, 278, 421, 17158, 29918, 8172, 29918, 1272, 2555, 740, 304, 9479, 5706, 20955, 411, 263, 3058, 6976, 29889, 1152, 1342, 29892, 366, 508, 3013, 5702, 310, 9251, 5759, 4206, 297, 263, 1051, 29892, 322, 769, 505, 263, 3058, 6976, 310, 7863, 263, 9251, 5759, 1948, 2012, 310, 14655, 263, 716, 1948, 29889, 2266, 29915, 29879, 385, 1342, 5314, 29901, 13, 28956, 1557, 893, 13, 5215, 4036, 13, 3166, 12865, 1053, 12865, 29892, 5335, 287, 2554, 13, 13, 29937, 22402, 8857, 310, 1950, 1819, 13, 1727, 1080, 353, 6024, 3308, 742, 525, 19960, 742, 525, 2287, 742, 525, 15860, 742, 525, 26016, 2033, 13, 29887, 21043, 353, 6024, 29924, 744, 742, 525, 29943, 331, 744, 2033, 13, 1179, 353, 1051, 29898, 3881, 29898, 29896, 29947, 29892, 29871, 29953, 29953, 876, 13, 262, 26807, 353, 1051, 29898, 3881, 29898, 29906, 29900, 29900, 29900, 29900, 29892, 29871, 29896, 29900, 29900, 29900, 29900, 29896, 29892, 29871, 29896, 29900, 29900, 29900, 29900, 876, 13, 3034, 2410, 29918, 4882, 267, 353, 6024, 15771, 742, 525, 7083, 1255, 2033, 13, 287, 1682, 800, 353, 6024, 16382, 4523, 742, 525, 1625, 4424, 742, 525, 3338, 13467, 403, 2033, 13, 4704, 29918, 20683, 353, 6024, 6821, 6046, 742, 525, 6638, 3842, 2033, 13, 13, 29937, 22402, 740, 304, 5706, 4036, 10116, 13, 1753, 4036, 29918, 1256, 29898, 2962, 29892, 1095, 1125, 13, 1678, 736, 1369, 718, 5335, 287, 2554, 29898, 23128, 29922, 8172, 29889, 9502, 524, 29898, 29900, 29892, 938, 3552, 355, 448, 1369, 467, 7827, 29918, 23128, 580, 4961, 13, 13, 29937, 22402, 740, 304, 5706, 4036, 848, 411, 1950, 848, 28410, 5626, 322, 20955, 13, 1753, 5706, 29918, 8172, 29918, 1272, 29898, 24957, 29918, 5727, 29892, 7929, 29918, 22795, 3097, 1125, 13, 1678, 565, 4036, 29889, 8172, 580, 529, 7929, 29918, 22795, 3097, 322, 3517, 29918, 5727, 29901, 13, 4706, 736, 3517, 29918, 5727, 14352, 29896, 29962, 13, 1678, 5120, 353, 4036, 29889, 16957, 29898, 1727, 1080, 29897, 13, 1678, 2635, 29918, 2212, 1312, 353, 4036, 29918, 1256, 29898, 12673, 29898, 29906, 29900, 29906, 29900, 29892, 29871, 29896, 29892, 29871, 29896, 511, 12865, 29898, 29906, 29900, 29906, 29906, 29892, 29871, 29896, 29892, 29871, 29896, 8106, 710, 615, 603, 877, 29995, 29979, 19222, 29885, 19222, 29881, 1495, 13, 1678, 1833, 29918, 4925, 353, 4036, 29918, 1256, 29898, 12673, 29898, 29906, 29900, 29906, 29906, 29892, 29871, 29896, 29892, 29871, 29906, 511, 12865, 29898, 29906, 29900, 29906, 29906, 29892, 29871, 29941, 29892, 29871, 29941, 29896, 8106, 710, 615, 603, 877, 29995, 29979, 19222, 29885, 19222, 29881, 1495, 13, 1678, 3234, 29918, 1493, 287, 353, 4036, 29889, 16957, 29898, 4704, 29918, 20683, 29897, 13, 1678, 3234, 29918, 29886, 2458, 1463, 353, 3234, 29918, 1493, 287, 565, 4036, 29889, 8172, 580, 529, 29871, 29900, 29889, 29945, 1683, 6629, 13, 1678, 23346, 353, 4036, 29889, 16957, 29898, 29887, 21043, 29897, 13, 1678, 5046, 353, 4036, 29889, 16957, 29898, 1179, 29897, 565, 4036, 29889, 8172, 580, 529, 29871, 29900, 29889, 29929, 1683, 6213, 13, 1678, 17869, 353, 4036, 29889, 16957, 29898, 262, 26807, 29897, 565, 4036, 29889, 8172, 580, 529, 29871, 29900, 29889, 29947, 1683, 6629, 13, 1678, 1766, 2410, 29918, 4882, 353, 4036, 29889, 16957, 29898, 3034, 2410, 29918, 4882, 267, 29897, 565, 4036, 29889, 8172, 580, 529, 29871, 29900, 29889, 29955, 1683, 6213, 13, 1678, 9793, 353, 4036, 29889, 16957, 29898, 287, 1682, 800, 29897, 565, 4036, 29889, 8172, 580, 529, 29871, 29900, 29889, 29953, 1683, 6629, 13, 1678, 3234, 29918, 7320, 353, 4036, 29889, 16957, 29898, 4704, 29918, 20683, 29897, 13, 1678, 1948, 353, 518, 12803, 29892, 2635, 29918, 2212, 1312, 29892, 1833, 29918, 4925, 29892, 3234, 29918, 1493, 287, 29892, 3234, 29918, 29886, 2458, 1463, 29892, 23346, 29892, 5046, 29892, 17869, 29892, 1766, 2410, 29918, 4882, 29892, 9793, 29892, 3234, 29918, 7320, 29962, 13, 1678, 3517, 29918, 5727, 29889, 4397, 29898, 798, 29897, 13, 1678, 736, 1948, 13, 13, 29937, 3251, 403, 4036, 848, 322, 2436, 304, 16874, 934, 13, 24957, 29918, 5727, 353, 5159, 13, 20908, 5926, 29918, 22795, 3097, 353, 29871, 29900, 29889, 29896, 13, 2541, 1722, 877, 19274, 386, 7492, 29918, 1272, 29918, 2541, 29918, 12175, 29918, 392, 29918, 20908, 15815, 29889, 7638, 742, 525, 29893, 1495, 408, 285, 29901, 13, 1678, 4839, 353, 6024, 18457, 742, 525, 2539, 3650, 1312, 742, 525, 8897, 10731, 742, 525, 7566, 4533, 287, 742, 525, 7566, 349, 2458, 1463, 742, 525, 29954, 1581, 742, 525, 22406, 742, 525, 797, 2763, 742, 525, 7083, 2410, 16034, 742, 525, 29923, 29392, 742, 525, 7566, 17943, 2033, 13, 1678, 285, 29889, 3539, 29898, 3788, 29889, 7122, 29898, 6672, 29897, 718, 11297, 29876, 1495, 13, 1678, 363, 474, 297, 3464, 29898, 29896, 29900, 29900, 29900, 1125, 13, 4706, 848, 353, 5706, 29918, 8172, 29918, 1272, 29898, 24957, 29918, 5727, 29892, 7929, 29918, 22795, 3097, 29897, 13, 4706, 285, 29889, 3539, 29898, 3788, 29889, 7122, 29898, 1958, 29898, 710, 29892, 848, 876, 718, 11297, 29876, 1495, 13, 13, 2158, 877, 29216, 386, 7492, 8783, 411, 848, 28410, 5626, 322, 20955, 5759, 322, 7160, 408, 14710, 7492, 29918, 1272, 29918, 2541, 29918, 12175, 29918, 392, 29918, 20908, 15815, 29889, 7638, 1495, 13, 28956, 13, 797, 445, 4784, 775, 29892, 591, 3013, 5702, 310, 278, 9251, 5759, 4206, 297, 278, 421, 24957, 29918, 5727, 29952, 1051, 29892, 322, 505, 263, 3058, 6976, 310, 7863, 263, 9251, 5759, 1948, 2012, 310, 14655, 263, 716, 697, 29889, 450, 421, 20908, 5926, 29918, 22795, 3097]).int().cuda()
    result = tokenizer.decode(output, skip_special_tokens=True)
    print(result)

    return 1, 0, 0

    llm = AutoModelForCausalLM.from_pretrained(
        model, torch_dtype=torch.float16, trust_remote_code=trust_remote_code)
    if llm.config.model_type == "llama":
        # To enable padding in the HF backend.
        tokenizer.pad_token = tokenizer.eos_token
    llm = llm.cuda()

    input_num_tokens = []
    output_num_tokens = []
    start = time.perf_counter()
    for i in tqdm(range(len(requests))):
        prompt, prompt_len, output_len = requests[i]
        # Generate the sequences.
        input_ids = tokenizer(prompt, return_tensors="pt",
                              padding=True).input_ids
        llm_outputs = llm.generate(
            input_ids=input_ids.cuda(),
            do_sample=False,
            num_return_sequences=1,
            num_beams=1,
            temperature=1.0,
            top_p=1.0,
            use_cache=True,
            max_new_tokens=output_len,
        )
        # Include the decoding time.
        tokenizer.decode(llm_outputs[0], skip_special_tokens=True)
        input_num_tokens.append(len(input_ids[0]))
        output_num_tokens.append(len(llm_outputs[0]))


    end = time.perf_counter()
    return end - start, input_num_tokens, output_num_tokens



def main(args: argparse.Namespace):
    print(args)
    random.seed(args.seed)

    # Sample the requests.
    tokenizer = AutoTokenizer.from_pretrained(
        args.tokenizer, trust_remote_code=args.trust_remote_code)
    if args.dataset is None:
        # Synthesize a prompt with the given input length.
        prompt = "hi" * (args.input_len - 1)
        requests = [(prompt, args.input_len, args.output_len)
                    for _ in range(args.num_samples)]

    else:
        with open(args.dataset) as f:
            requests = json.load(f)

    if args.num_samples is not None:
        requests = requests[0:args.num_samples]

    elapsed_time, input_num_tokens, output_num_tokens = run_hf(requests, args.model, tokenizer,  args.trust_remote_code)
    # prompt_num_tokens = sum(prompt_len for prompt_len in input_num_tokens)
    # total_num_tokens = sum(output_len for output_len in output_num_tokens)
    # print(f"Throughput: {len(requests) / elapsed_time:.2f} requests/s \n"
    #       f"Tokens/s: {total_num_tokens / elapsed_time:.2f} tokens/s \n"
    #       f"Prompt_num_tokens:{prompt_num_tokens:.2f} tokens \n"
    #       f"Total_num_tokens:{total_num_tokens:.2f} tokens \n")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Benchmark the throughput.")
    parser.add_argument("--dataset", type=str, default=None, help="Path to the dataset.")
    parser.add_argument("--model", type=str, default="meta/llama2-70b")
    parser.add_argument("--tokenizer", type=str, default=None)
    parser.add_argument("--input-len", type=int, default=None, help="Input prompt length for each request")
    parser.add_argument("--output-len", type=int, default=None, help="Output length for each request")
    parser.add_argument("--num-samples", type=int, default=None, help="Number of first few samples used for inference test")
    parser.add_argument("--seed", type=int, default=0)
    parser.add_argument('--trust-remote-code',
                        action='store_true',
                        help='trust remote code from huggingface')
    parser.add_argument(
        '--dtype',
        type=str,
        default='auto',
        choices=['auto', 'half', 'float16', 'bfloat16', 'float', 'float32'],
        help='data type for model weights and activations. '
        'The "auto" option will use FP16 precision '
        'for FP32 and FP16 models, and BF16 precision '
        'for BF16 models.')
    args = parser.parse_args()
    if args.tokenizer is None:
        args.tokenizer = args.model
    if args.dataset is None:
        assert args.input_len is not None
        assert args.output_len is not None
    else:
        assert args.input_len is None
        assert args.output_len is None

    main(args)
